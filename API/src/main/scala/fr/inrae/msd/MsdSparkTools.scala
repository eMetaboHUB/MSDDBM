package fr.inrae.msd

import org.apache.spark.sql.{Dataset, SparkSession}

import scala.util.Try
import net.sansa_stack.rdf.spark.io._
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.jena.graph
import org.apache.jena.graph.{Node, NodeFactory, Triple}
import org.apache.jena.riot.Lang
import org.apache.spark.rdd.RDD
import sbtBuildInfo.BuildInfo
import org.apache.jena.vocabulary._
import net.sansa_stack.inference.spark.forwardchaining.triples._
import org.apache.spark.sql
import org.apache.spark.util.SizeEstimator

import java.io.FileNotFoundException


/** Contains a set of Spark functions. Do not use outside of a Spark context.
 *
 * @param configFile Path to the config file (see example file src/main/resources/msdQuery.conf)
 */
case class MsdSparkTools(configFile: String) {

  val DEFAULT_MAX_RECORDS_PER_FILE = 1000000

  @transient lazy val spark: SparkSession = SparkSession.builder().getOrCreate()

  val apiVersion: String = BuildInfo.version
  @transient private lazy val fileSystem: FileSystem = FileSystem.get(spark.sparkContext.hadoopConfiguration)
  @transient private lazy val dataLake = new DataLake(configFile)


  /** Adds domains and ranges to VoID property partition, with namespace http://inrae.fr/voidext/
   * Reads the VoID file generated by distLODStats during ingestion, loads every available ontologies and infers available ranges and domains.
   * Then saves the old void and writes the new one in Ntriple format.
   *
   */
  def enhanceVoid(graphName: String,
                  graphVersion: String,
                  outputDir: String,
                  parallelism: Int = 48,
                  maxOntologySize: Long = 5000,
                  useHorstReasoner: Boolean = false): Unit = {


    if (!hasCompatibleRdfFormat(graphName, graphVersion)) {
      dataLake.log(s"Graph ${graphName} format is not RDF or not recognised by Jena. Skipping VoID enhancement")
      return
    }
    dataLake.log(s"VoID enhancement START for  ${graphName} ${graphVersion}")

    val voidTriples = spark.rdf(Lang.TURTLE)(dataLake.getVoidDefaultOutputDir(graphName, graphVersion))
    val graphTriples = getGraphContent(graphName, graphVersion)

    val filteredOntologies = dataLake.getLatestOntologies
      .filter(g => Void(dataLake.getGraphStats(g.dirname, g.graphVersion)).getGraphSize <= maxOntologySize)
    dataLake.log(s"VoID enhancement for  ${graphName} ${graphVersion} using ontologies less than ${maxOntologySize} triples : ")
    filteredOntologies.map("using ontology " + _.dirname).foreach(s => dataLake.log(s))
    val ontologiesTriples = getGraphContent(filteredOntologies)

    val reasoner = if (useHorstReasoner) {
      dataLake.log("Using Horst OWL reasoner")
      new ForwardRuleReasonerOWLHorst(spark.sparkContext, parallelism)
    }
    else {
      dataLake.log("Using simple RDFS reasoner")
      new ForwardRuleReasonerRDFSDataframe(spark, parallelism)
    }

    //TODO After this line, println wont work...
    val inferredTriples: RDD[Triple] = reasoner.apply(voidTriples.union(ontologiesTriples).union(graphTriples))

    def getRangesOrDomainsKeyedByProperty(f: Triple => Boolean): RDD[(Node, Node)] = {
      inferredTriples.filter(t => f(t))
        .map(t => (t.getSubject, t.getObject))
    }

    val rangesKeyedByProperty = getRangesOrDomainsKeyedByProperty(triple => triple.getPredicate == RDFS.range.asNode() || triple.getPredicate.toString() == "http://purl.org/dc/dcam/rangeIncludes")
    val domainsKeyedByProperty = getRangesOrDomainsKeyedByProperty(triple => triple.getPredicate == RDFS.domain.asNode() || triple.getPredicate.toString() == "http://purl.org/dc/dcam/domainIncludes")

    val voidTriplesKeyedByProperty: RDD[(Node, (Node, Node))] = voidTriples.map {
      t => (t.getObject, (t.getSubject, t.getPredicate))
    }

    val joinedVoidAndRanges: RDD[(Node, ((Node, Node), Node))] = voidTriplesKeyedByProperty.join(rangesKeyedByProperty)
    val joinedVoidAndDomains: RDD[(Node, ((Node, Node), Node))] = voidTriplesKeyedByProperty.join(domainsKeyedByProperty)

    val newRangeVoidTriples: RDD[Triple] = joinedVoidAndRanges.map {
      case (_, ((entity, _), range)) => Triple.create(entity, NodeFactory.createURI("http://inrae.fr/voidext/range"), range)
    }

    val newDomainVoidTriples: RDD[Triple] = joinedVoidAndDomains.map {
      case (_, ((entity, _), range)) => Triple.create(entity, NodeFactory.createURI("http://inrae.fr/voidext/domain"), range)
    }

    val voidDir = if (outputDir == "") dataLake.getVoidDefaultOutputDir(graphName, graphVersion) else outputDir
    //Because of Spark laziness, we have to run the job before move the source file we want to replace
    val millis = System.currentTimeMillis()
    newRangeVoidTriples.union(newDomainVoidTriples).union(voidTriples).saveAsNTriplesFile(voidDir + s".$millis.new")
    backupDir(voidDir)
    dataLake.fileSystem.rename(new Path(voidDir + s".$millis.new"), new Path(voidDir))
    dataLake.log(s"VoID enhancement END for  ${graphName} ${graphVersion}")
  }

  /** Launches the computation of Vocabulary of Interlinked Datasets (VoID) statistics of a graph, and writes it to a specified directory
   *
   */
  def void(graphName: String, graphVersion: String, outputDir: String): Unit = {

    def runVoid(outputDir: String, triples: RDD[org.apache.jena.graph.Triple], source: String): Unit = {
      dataLake.log(s"VoID  START to  $outputDir")
      val stats = MsdRDFStatistics.run(triples)
      MsdRDFStatistics.voidify(stats, source, outputDir)
      dataLake.log(s"VoID  DONE : SEE $outputDir")
    }

    if (!hasCompatibleRdfFormat(graphName, graphVersion)) {
      dataLake.log(s"Graph ${graphName} format is not RDF or not recognised by Jena. Skipping VoID")
      return
    }

    val graph = dataLake.getGraph(graphName, graphVersion)
    val triples = getGraphContent(graphName, graphVersion)
    val globalOutputDir = if (outputDir == "") dataLake.getVoidDefaultOutputDir(graphName, graphVersion) else outputDir

    backupDir(globalOutputDir)
    runVoid(globalOutputDir, triples, graph.graphDirPath)
    val model = Try(Void(dataLake.getGraphStats(graphName, graphVersion)))
    if (model.isFailure) {
      dataLake.log(s"Void generated for ${graphName} ${graphVersion} cannot be parsed by Jena. Throwing... ")
      throw model.failed.get
    }
  }


  /** Gets the triples of multiple graphs in an RDD.
   *
   * @param graphs A sequence of knowledge graphs to be loaded
   * @return A single RDD containing the graphs' triples
   */
  def getGraphContent(graphs: Seq[KnowledgeGraph]): RDD[graph.Triple] = {
    val rdds = graphs.map(g => getGraphContent(g.dirname, g.graphVersion))
    if (rdds.isEmpty)
      spark.sparkContext.emptyRDD[graph.Triple]
    else
      rdds.reduce(_.union(_))
  }

  /** Gets parts of the content of a graph as an RDD[Triple] based on a list of file.
   *
   * @param dataLake     Will be fed by the runSparkJob function
   * @param sparkSession Will be fed by the runSparkJob function
   * @param graphName    name of the graph to be loaded
   * @param graphVersion version of the graph to be loaded, empty string for latest version
   * @param fileList     list of files to load
   * @return a resilient distributed dataset containing the whole graph's triples
   */
  def getGraphContent(graphName: String, graphVersion: String, fileList: Seq[String]): RDD[graph.Triple] = {
    val graph = dataLake.getGraph(graphName, graphVersion)
    val format = Try(getJenaLangFromString(graph.strRdfFormat))
    if (format.isFailure) {
      dataLake.log(s"$graphName with format ${graph.strRdfFormat} cannot be parsed by Jena. Skipping.")
      throw format.failed.get
    }
    val fileListToLoad = graph.getFiles.filter(file => fileList.isEmpty || fileList.contains(file))
    spark.rdf(format.get)(fileListToLoad.mkString(","))
  }

  /** Gets the content of a graph as an RDD[Triple]
   *
   * @param graphName    name of the graph to be loaded
   * @param graphVersion version of the graph to be loaded, empty string for latest version
   * @param forceParquet if set, throw an exception when trying to load a graph that hasn't be converted to parquet
   * @return a resilient distributed dataset containing the whole graph's triples
   */
  def getGraphContent(graphName: String, graphVersion: String, forceParquet: Boolean = false): RDD[graph.Triple] = {
    //getGraphContent(graphName, graphVersion, Seq.empty[String])
    val graph = dataLake.getGraph(graphName, graphVersion)
    val format = Try(getJenaLangFromString(graph.strRdfFormat))
    if (format.isFailure) {
      dataLake.log(s"$graphName with format ${graph.strRdfFormat} cannot be parsed by Jena. Skipping.")
      throw format.failed.get
    }

    val inputPath = graph.graphDirPath.stripSuffix("/") + ".parquet"
    val parquetAvailable = dataLake.fileSystem.exists(new Path(inputPath)) //listStatus(new Path(inputPath)).map(_.getPath.toString).contains(inputPath)
    if (parquetAvailable) {
      loadTriplesFromParquet(inputPath)
    } else {
      if (forceParquet) throw new FileNotFoundException(s"Could not find graph $graphName $graphVersion parquet directory ($inputPath), athough forceParquet option is set.")
      spark.rdf(format.get)(graph.getFiles.mkString(","))
    }
  }


  /** Converts a string to a Lang object
   *
   * @param format rdf format as a String, eg. "TURTLE", "NTRIPLES", "RDFXML"
   * @return rdf format as Jena Lang object
   */
  def getJenaLangFromString(format: String): org.apache.jena.riot.Lang = {
    classOf[org.apache.jena.riot.Lang]
      .getField(format)
      .get()
      .asInstanceOf[org.apache.jena.riot.Lang]
  }


  /** Helper for SparFunctions. backups existent output dir
   *
   * @param dataLake        Will be fed by the runSparkJob function
   * @param sparkSession    Will be fed by the runSparkJob function
   * @param targetDirectory Name of the directory to be checked
   */
  private def backupDir(targetDirectory: String): Unit = {

    if (targetDirectory != "") {
      val dirPath = new Path(targetDirectory)
      if (dataLake.fileSystem.exists(dirPath)) {
        val newDirName = targetDirectory + System.currentTimeMillis() + ".bak"
        dataLake.log(s"Warning : $targetDirectory already exists, Moving to $newDirName ")
        dataLake.fileSystem.rename(dirPath, new Path(newDirName))
      }
    }

  }

  /** Tests the compatibility of a graph format with Jena.
   *
   * @param graphName    graph name
   * @param graphVersion graph version
   * @return True if the graph is Jena-compatible, false otherwise.
   */
  private def hasCompatibleRdfFormat(graphName: String, graphVersion: String): Boolean = {
    val graph = dataLake.getGraph(graphName, graphVersion)
    val format = Try(getJenaLangFromString(graph.strRdfFormat))
    format.isSuccess
  }


  def serializeGraphToParquet(graphName: String, graphVersion: String, maxTriplesPerFile : Long = DEFAULT_MAX_RECORDS_PER_FILE): Unit = {
    val graph = dataLake.getGraph(graphName, graphVersion)
    val outputPath = graph.graphDirPath.stripSuffix("/") + ".parquet"
    val outputHDFSPath = new Path(outputPath)
    if (dataLake.fileSystem.exists(outputHDFSPath)) {
      val timestamp = System.currentTimeMillis()
      val backupPath = new Path(outputPath + "_" + timestamp)
      dataLake.fileSystem.rename(outputHDFSPath, backupPath)
    }
    val graphTriples = getGraphContent(graphName, graphVersion)
    saveTriplesAsParquet(graphTriples, outputPath, maxTriplesPerFile)
  }

  def saveTriplesAsParquet(rddTriple: RDD[Triple], path: String, maxTriplesPerFile : Long = DEFAULT_MAX_RECORDS_PER_FILE ): Unit = {
    import spark.implicits._

    val tripleCase = rddTriple.map(triple => TripleCase(
      triple.getSubject.toString,
      triple.getPredicate.toString,
      triple.getObject.toString
    )).toDF()

    tripleCase
      .write
      .option("maxRecordsPerFile", maxTriplesPerFile)
      .parquet(path)
  }

  def loadTriplesFromParquet(path: String): RDD[Triple] = {
    val df = spark.read.parquet(path)
    import spark.implicits._
    val rddTripleCase = df.as[TripleCase].rdd

    rddTripleCase.map(tripleCase => Triple.create(
      NodeFactory.createURI(tripleCase.subject),
      NodeFactory.createURI(tripleCase.predicate),
      NodeFactory.createURI(tripleCase.obj)
    ))
  }


//  def repartition[T](graph: sql.Dataset[T], partitionSizeMb: Int, testContext: Boolean=false): sql.Dataset[T] = {
//    val memUsage = if (!testContext) estimateDatasetSize(graph, 0.05) else estimateDatasetSize(graph, 1)
//    val nPartitions = estimatePartitionNumber(memUsage, partitionSizeMb)
//    println(s"MSDLib : Repartitioning. Number of partitions = $nPartitions ")
//    graph.repartition(nPartitions)
//  }
//
//  def estimateDatasetSize[T](dataset: Dataset[T], sampleFraction: Double = 0.05): Long = {
//    val sampleDataset = dataset.sample(withReplacement = false, sampleFraction)
//    // Cache the sample to ensure accurate size estimation
//    sampleDataset.cache()
//    sampleDataset.count() // Materialize the cache
//    val sampleSizeInBytes = sampleDataset.queryExecution.optimizedPlan.stats.sizeInBytes
//    sampleDataset.unpersist()
//    val totalSizeInBytes = (sampleSizeInBytes.toDouble / sampleFraction).toLong
//    totalSizeInBytes
//  }

//  private def estimateGraphMemoryUsage[T](graph: sql.Dataset[T], sampleFraction: Double): Long = {
//    val sample = graph.sample(withReplacement = false, sampleFraction)
//    val sampleCount = sample.count()
//    val totalCount = graph.count()
//    val sampleSize = sample.mapPartitions { iter =>
//      Iterator(SizeEstimator.estimate(iter.toList))
//    }
//    (sampleSize * (totalCount.toDouble / sampleCount.toDouble)).toLong
//  }

//  def repartition[T](graph: RDD[T], partitionSizeMb: Int, testContext: Boolean=false): RDD[T] = {
//    val memUsage = if (!testContext) estimateGraphMemoryUsage(graph, 0.05) else estimateGraphMemoryUsage(graph, 1)
//    val nPartitions = estimatePartitionNumber(memUsage, partitionSizeMb)
//    println(s"MSDLib : Repartitioning. Number of partitions = $nPartitions ")
//    graph.repartition(nPartitions)
//  }

//  private def estimateGraphMemoryUsage[T](graph: RDD[T], sampleFraction: Double): Long = {
//    val sampleRDD = graph.sample(withReplacement = false, sampleFraction)
//    val sampleCount = sampleRDD.count()
//    val totalCount = graph.count()
//    val sampleSize = sampleRDD.mapPartitions { iter =>
//      Iterator(SizeEstimator.estimate(iter.toList))
//    }.sum
//    (sampleSize * (totalCount.toDouble / sampleCount.toDouble)).toLong
//  }
//
//  private def estimatePartitionNumber(memUsage: Long, partitionSizeMb: Int): Int = {
//    val maxPartitionSize = partitionSizeMb * 1024 * 1024
//    Math.max(1, Math.ceil(memUsage / maxPartitionSize).toInt)
//  }


}

case class TripleCase(subject: String, predicate: String, obj: String)

